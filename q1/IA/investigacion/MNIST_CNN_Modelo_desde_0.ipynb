{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_tlJe_LxpcP"
      },
      "source": [
        "Aquí se importan torch, torchvision para manipular los datos, y matplotlib para la visualización."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Hjw6gx4ViCot"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import torch.nn as nn\n",
        "\n",
        "from torch import optim\n",
        "\n",
        "from torch.autograd import Variable"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ewKmveW2x4m6"
      },
      "source": [
        "Utiliza una GPU ('cuda') si está disponible; de lo contrario, recurre a la CPU ('cpu'). ###\n",
        "\n",
        "A diferencia de tf, aqui hay que espacificar que se tiene que usar la GPU, en tf se utiliza directamente la GPU si se detecta."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "51FVRGw0imkv",
        "outputId": "db703d59-87c1-4991-8abf-555ebaedcaa8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WVJSGMRy0YzO"
      },
      "source": [
        "Carga los datos de train y test en un nuevo directorio 'data'.\n",
        "\n",
        "Es el equivalente a hacer en tf lo siguiente:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "from tensorflow.keras.datasets import mnist\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "```\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9efa-6QBimw8",
        "outputId": "e9eb4978-4c21-47d9-bd85-edea6a482c1c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 421203270.73it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting data/MNIST/raw/train-images-idx3-ubyte.gz to data/MNIST/raw\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 58718222.89it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting data/MNIST/raw/train-labels-idx1-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 170838678.84it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting data/MNIST/raw/t10k-images-idx3-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 22255290.62it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting data/MNIST/raw/t10k-labels-idx1-ubyte.gz to data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "train_data = datasets.MNIST(\n",
        "    root = 'data',\n",
        "    train = True,\n",
        "    transform = ToTensor(),\n",
        "    download = True,\n",
        ")\n",
        "test_data = datasets.MNIST(\n",
        "    root = 'data',\n",
        "    train = False,\n",
        "    transform = ToTensor()\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2YP1owZ32PLe"
      },
      "source": [
        "Se utiliza '.size()' para ver las dimensiones de train y test."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JwGVqTskimzv",
        "outputId": "4b9c69a5-41b2-459e-d88b-71f03cbbe1eb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset MNIST\n",
            "    Number of datapoints: 60000\n",
            "    Root location: data\n",
            "    Split: Train\n",
            "    StandardTransform\n",
            "Transform: ToTensor()\n",
            "torch.Size([60000, 28, 28])\n"
          ]
        }
      ],
      "source": [
        "print(train_data)\n",
        "print(train_data.data.size())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ouSdVqTUim2N",
        "outputId": "0aba8697-ba95-45e8-89ca-3c2c732a1f69"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset MNIST\n",
            "    Number of datapoints: 10000\n",
            "    Root location: data\n",
            "    Split: Test\n",
            "    StandardTransform\n",
            "Transform: ToTensor()\n",
            "torch.Size([10000, 28, 28])\n"
          ]
        }
      ],
      "source": [
        "print(test_data)\n",
        "print(test_data.data.size())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wWlw--ke3NCI"
      },
      "source": [
        "Para cargar los datos se utiliza el '.DataLoader()', donde podemos especificar la cantidad de imagenes que habrá en cada lote. Además con num_workers se puede especificar la cantidad e subprocesos que se usarán para la carga de datos, a más 'trabajadores' mas velocidad al cargar los datos y mas carga en la CPU. Además una sobrecarga de 'trabajadores' puede llevar a un bloqueo de memoria.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qLChOstUmfuG",
        "outputId": "02906424-e99f-433c-9491-76eba295a6ba"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'train': <torch.utils.data.dataloader.DataLoader at 0x7effa00fe860>,\n",
              " 'test': <torch.utils.data.dataloader.DataLoader at 0x7effa00fcb50>}"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "loaders = {\n",
        "    'train' : torch.utils.data.DataLoader(train_data,\n",
        "                                          batch_size=100,\n",
        "                                          shuffle=True,\n",
        "                                          num_workers=1),\n",
        "\n",
        "    'test'  : torch.utils.data.DataLoader(test_data,\n",
        "                                          batch_size=100,\n",
        "                                          shuffle=True,\n",
        "                                          num_workers=1),\n",
        "}\n",
        "loaders"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5hjWgp4n6HMM"
      },
      "source": [
        "Para poder hacer una red neuronal en pytorch tenemos que hacer una clase, esta debe de heredar de 'nn.Module', clase de la que derivan todas las redes neuronales de PyTorch. Con 'super(Modelo_CNN, self).__init__():' se lama al constructor de la clase padre.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Con 'nn.Conv2d()' se crea una capa conolucional 2D. En esta se especifica que la entrada tiene un canal (en este caso por se una escala de grises, si fuese una imagen RGB -> 'in_channels = 3'), se la aplicarán 16 filtros y que el tamaño del kernel es de 5*5.\n",
        "\n",
        "Despues aplica la función de activación ReLU y hace un Maxpooling.\n",
        "\n",
        "La siguiente capa es practicamente lo mismo, tan solo se cambia en número de filtros a 32.  \n",
        "\n",
        "'Linear()' es una capa lineal que espera que sus entradas ya estén aplanadas, y el aplanamiento se maneja explícitamente en el método forward. Linear en PyTorch es lo mismo que '.Dense()' en keras. 10 es el numero de clases de salida que hay (numeros del 0 al 9). En este caso, hay 32 canales, cada uno de tamaño 7x7 (28/2-> 14/2-> 7), por lo tanto 32 * 7 * 7.\n",
        "\n",
        " 32 * 7 * 7 representa el número total de elementos en el tensor aplanado que llegan a la capa lineal con 10 salidas\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "El método forward en una clase de red neuronal en PyTorch define cómo los datos de entrada (en este caso x) fluyen a través de la red.\n",
        "\n",
        "'x' es el tensor de entrada que pasa por la capa conv1 y conv2. Antes de pasar 'x' a la capa lineal es necesario aplanarlo en un vector de una única dimensión.\n",
        "'x.view(x.size(0), -1)' realiza esta operación. 'x.size(0)' es el tamaño del batch, y -1 le dice a PyTorch que calcule automáticamente la dimensión necesaria para que el aplanamiento sea correcto. En este caso, 32 * 7 * 7. Despues de esto, 'x' ya puede pasar por la capa .out para la que era necesario tener un vector aplanado.\n",
        "\n",
        "Al final este método devuelve la predicción ('output') y el tensor 'x'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "-5HQssgimvQm"
      },
      "outputs": [],
      "source": [
        "class Modelo_CNN(nn.Module): # Define una nueva clase de red neuronal. Modelo_CNN\n",
        "    def __init__(self):\n",
        "      super(Modelo_CNN, self).__init__() # Llama al constructor de la clase padre para realizar la inicialización necesaria.\n",
        "      # Primera capa convolucional\n",
        "      self.conv1 = nn.Sequential(\n",
        "          nn.Conv2d(\n",
        "              in_channels=1,\n",
        "              out_channels=16,\n",
        "              kernel_size=5,\n",
        "              stride=1,\n",
        "              padding=2\n",
        "          ),\n",
        "          nn.ReLU(),\n",
        "          nn.MaxPool2d(kernel_size=2)\n",
        "      )\n",
        "      self.conv2= nn.Sequential(\n",
        "          nn.Conv2d(16, 32, 5, 1, 2),\n",
        "          nn.ReLU(),\n",
        "          nn.MaxPool2d(2)\n",
        "      )\n",
        "      self.out = nn.Linear(32 * 7 * 7, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        output = self.out(x)\n",
        "        return output, x\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NApz6PXBGUGh"
      },
      "source": [
        "Una vez está creado el modelo, podemos llamar a la función para que nos instancie un objeto 'Modelo_CNN()' y si hacemos un print pordemos ver las características del modelo\n",
        "\n",
        "Es como el '.summary()'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3BoJ9ylap9h0",
        "outputId": "8629b62c-1822-4907-db53-9ba24a24d5cc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Modelo_CNN(\n",
            "  (conv1): Sequential(\n",
            "    (0): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "    (1): ReLU()\n",
            "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (conv2): Sequential(\n",
            "    (0): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "    (1): ReLU()\n",
            "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (out): Linear(in_features=1568, out_features=10, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "mi_modelo = Modelo_CNN()\n",
        "print(mi_modelo)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7MAW0xFNqv0"
      },
      "source": [
        "Se epecifica la función de perdida y el optimizador."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RZ06lN3CqQOq",
        "outputId": "7bf3c61a-ab86-43ed-f080-5a7a31e50114"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "CrossEntropyLoss()"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "loss_func = nn.CrossEntropyLoss()\n",
        "loss_func"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ggynsMAXqc65",
        "outputId": "ed868204-2e6e-497b-ab30-13b3b53d07bc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Adam (\n",
              "Parameter Group 0\n",
              "    amsgrad: False\n",
              "    betas: (0.9, 0.999)\n",
              "    capturable: False\n",
              "    differentiable: False\n",
              "    eps: 1e-08\n",
              "    foreach: None\n",
              "    fused: None\n",
              "    lr: 0.01\n",
              "    maximize: False\n",
              "    weight_decay: 0\n",
              ")"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "optimizer = optim.Adam(mi_modelo.parameters(), lr = 0.01)\n",
        "optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tGuZ8SfPRQ1R"
      },
      "source": [
        "Para entrenar el modolo hay que hacer una función que se encargue de ello. Recibe el número de épocas que se va entrenar el modelo, el modelo a entrenar y un diccionario que contiene los DataLoaders para los conjuntos de datos de entrenamiento.\n",
        "\n",
        "---\n",
        "\n",
        "'cnn.train()' pone el modelo en modo entrenamiento, es importante para capas como dropout o batch normalization, que tienen comportamientos diferentes durante el entrenamiento y la evaluación.\n",
        "\n",
        "Luego, en cada época el modelo itera sobre cada lote de datos (images y labels) en el DataLoader de entrenamiento.\n",
        "\n",
        "\n",
        "'cnn(b_x)[0]' pasa el lote de imágenes a través del modelo y obtiene las salidas.\n",
        "'loss_func(output, b_y)' calcula la pérdida comparando la salida del modelo con la etiqueta.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Duxkt1tAq4vV",
        "outputId": "4bc190a4-0e5b-4e8c-ae3e-47a68c793630"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/5], Step [100/600], Loss: 0.0654\n",
            "Epoch [1/5], Step [200/600], Loss: 0.0664\n",
            "Epoch [1/5], Step [300/600], Loss: 0.0027\n",
            "Epoch [1/5], Step [400/600], Loss: 0.0813\n",
            "Epoch [1/5], Step [500/600], Loss: 0.0059\n",
            "Epoch [1/5], Step [600/600], Loss: 0.0078\n",
            "Epoch [2/5], Step [100/600], Loss: 0.0258\n",
            "Epoch [2/5], Step [200/600], Loss: 0.0555\n",
            "Epoch [2/5], Step [300/600], Loss: 0.0134\n",
            "Epoch [2/5], Step [400/600], Loss: 0.2154\n",
            "Epoch [2/5], Step [500/600], Loss: 0.0691\n",
            "Epoch [2/5], Step [600/600], Loss: 0.0214\n",
            "Epoch [3/5], Step [100/600], Loss: 0.0279\n",
            "Epoch [3/5], Step [200/600], Loss: 0.0936\n",
            "Epoch [3/5], Step [300/600], Loss: 0.0029\n",
            "Epoch [3/5], Step [400/600], Loss: 0.0735\n",
            "Epoch [3/5], Step [500/600], Loss: 0.1277\n",
            "Epoch [3/5], Step [600/600], Loss: 0.0709\n",
            "Epoch [4/5], Step [100/600], Loss: 0.1238\n",
            "Epoch [4/5], Step [200/600], Loss: 0.0762\n",
            "Epoch [4/5], Step [300/600], Loss: 0.0220\n",
            "Epoch [4/5], Step [400/600], Loss: 0.0519\n",
            "Epoch [4/5], Step [500/600], Loss: 0.0266\n",
            "Epoch [4/5], Step [600/600], Loss: 0.0062\n",
            "Epoch [5/5], Step [100/600], Loss: 0.0626\n",
            "Epoch [5/5], Step [200/600], Loss: 0.0017\n",
            "Epoch [5/5], Step [300/600], Loss: 0.0042\n",
            "Epoch [5/5], Step [400/600], Loss: 0.0500\n",
            "Epoch [5/5], Step [500/600], Loss: 0.0188\n",
            "Epoch [5/5], Step [600/600], Loss: 0.0131\n"
          ]
        }
      ],
      "source": [
        "num_epochs = 5\n",
        "\n",
        "def train(num_epochs, cnn, loaders):\n",
        "\n",
        "    cnn.train()\n",
        "\n",
        "    total_step = len(loaders['train'])\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        for i, (images, labels) in enumerate(loaders['train']):\n",
        "\n",
        "            b_x = Variable(images)   # batch x\n",
        "            b_y = Variable(labels)   # batch y\n",
        "            output = cnn(b_x)[0]\n",
        "            loss = loss_func(output, b_y)\n",
        "\n",
        "            optimizer.zero_grad()   # limpia los gradientes antiguos\n",
        "            loss.backward()         # calcula los gradientes\n",
        "            optimizer.step()        # actualiza los parámetros\n",
        "\n",
        "            if (i+1) % 100 == 0:\n",
        "                print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'.format(epoch + 1, num_epochs, i + 1, total_step, loss.item()))\n",
        "            pass\n",
        "        pass\n",
        "    pass\n",
        "\n",
        "train(num_epochs, mi_modelo, loaders)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "moz09Gc7Vr6b"
      },
      "source": [
        "Esta función es la que comprobará la eficacia de la CNN."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A2rg1vXEVagy",
        "outputId": "a963b227-b9c4-47d3-fb5a-14d6805a7eb7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Precisión del modelo en el conjunto de datos de prueba: 97.71%\n"
          ]
        }
      ],
      "source": [
        "def test(modelo, cargador_de_pruebas):\n",
        "    # Poner el modelo en modo de evaluación\n",
        "    modelo.eval()\n",
        "\n",
        "    # Inicializar contadores de predicciones correctas y total de muestras\n",
        "    correctas = 0\n",
        "    total = 0\n",
        "\n",
        "    # Desactivar el cálculo de gradientes\n",
        "    with torch.no_grad():\n",
        "        # Iterar sobre el conjunto de datos de prueba\n",
        "        for imagenes, etiquetas in cargador_de_pruebas:\n",
        "            # Realizar predicciones\n",
        "            salidas, _ = modelo(imagenes)\n",
        "            # Obtener la clase predicha para cada imagen\n",
        "            _, predicciones = torch.max(salidas, 1)\n",
        "\n",
        "            # Contar predicciones correctas y el total de muestras\n",
        "            correctas += (predicciones == etiquetas).sum().item()\n",
        "            total += etiquetas.size(0)\n",
        "\n",
        "    # Calcular la precisión\n",
        "    precision = 100 * correctas / total\n",
        "    print(f'Precisión del modelo en el conjunto de datos de prueba: {precision:.2f}%')\n",
        "    return precision\n",
        "\n",
        "# Llamada a la función test\n",
        "precision = test(mi_modelo, loaders['test'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L_RybzYUIU5b"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
