{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9c7oa5KeOQ2s"
   },
   "source": [
    "Importo las librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "w4RBuI1MpaCI"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BsQbrQhTORS7"
   },
   "source": [
    "MyModel se utiliza para definir un modelo de lenguaje recurrente (RNN) básico en TensorFlow usando Keras. Este tipo de modelo se suele utilizar para tareas relacionadas con el procesamiento de lenguaje natural, en este caso, de generación de texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "hiHaOH5prK0W"
   },
   "outputs": [],
   "source": [
    "class MyModel(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
    "    super().__init__(self)\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "    self.gru = tf.keras.layers.GRU(rnn_units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True)\n",
    "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "  def call(self, inputs, states=None, return_state=False, training=False):\n",
    "    x = inputs\n",
    "    x = self.embedding(x, training=training)\n",
    "    if states is None:\n",
    "      states = self.gru.get_initial_state(x)\n",
    "    x, states = self.gru(x, initial_state=states, training=training)\n",
    "    x = self.dense(x, training=training)\n",
    "\n",
    "    if return_state:\n",
    "      return x, states\n",
    "    else:\n",
    "      return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EBCbBqm5OR4U"
   },
   "source": [
    "OneStep se utiliza para definir un modelo de generación de texto de un solo paso basado en un modelo más grande. Genera un único carácter en función de una entrada dada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "aGmuwhcurMFc"
   },
   "outputs": [],
   "source": [
    "class OneStep(tf.keras.Model):\n",
    "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
    "    super().__init__()\n",
    "    self.temperature = temperature\n",
    "    self.model = model\n",
    "    self.chars_from_ids = chars_from_ids\n",
    "    self.ids_from_chars = ids_from_chars\n",
    "\n",
    "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
    "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
    "    sparse_mask = tf.SparseTensor(\n",
    "        # Put a -inf at each bad index.\n",
    "        values=[-float('inf')]*len(skip_ids),\n",
    "        indices=skip_ids,\n",
    "        # Match the shape to the vocabulary\n",
    "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
    "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
    "\n",
    "  @tf.function\n",
    "  def generate_one_step(self, inputs, states=None):\n",
    "    # Convert strings to token IDs.\n",
    "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
    "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
    "\n",
    "    # Run the model.\n",
    "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
    "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
    "                                          return_state=True)\n",
    "    # Only use the last prediction.\n",
    "    predicted_logits = predicted_logits[:, -1, :]\n",
    "    predicted_logits = predicted_logits/self.temperature\n",
    "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
    "    predicted_logits = predicted_logits + self.prediction_mask\n",
    "\n",
    "    # Sample the output logits to generate token IDs.\n",
    "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
    "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
    "\n",
    "    # Convert from token ids to characters\n",
    "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
    "\n",
    "    # Return the characters and model state.\n",
    "    return predicted_chars, states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MoUoZFhkOS2V"
   },
   "source": [
    "Se cargan los modelos desde google drive o la carpeta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "0l_L9aRErQlu"
   },
   "outputs": [],
   "source": [
    "one_step_abascal = tf.saved_model.load('one_step_abascal')\n",
    "one_step_casado = tf.saved_model.load('one_step_casado')\n",
    "one_step_sanchez = tf.saved_model.load('one_step_sanchez')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9oMzZGywOTZe"
   },
   "source": [
    "La función encapsula la lógica de generación de texto utilizando un modelo dado. Se puede utilizar esta función para generar respuestas de texto dada una semilla y la longitud deseada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "iiDJj46kBJGb"
   },
   "outputs": [],
   "source": [
    "def generar_respuesta(modelo, semilla, longitud=300):\n",
    "    states = None\n",
    "    next_char = tf.constant([semilla])\n",
    "    result = [next_char]\n",
    "\n",
    "    for _ in range(longitud):\n",
    "        next_char, states = modelo.generate_one_step(next_char, states=states)\n",
    "        result.append(next_char)\n",
    "\n",
    "    result = tf.strings.join(result)\n",
    "    respuesta_completa = result[0].numpy().decode('utf-8')\n",
    "\n",
    "    # Obtener el primer índice después de la semilla\n",
    "    primer_indice_despues_de_semilla = len(semilla)\n",
    "\n",
    "    # Obtener todos los caracteres a partir del primer índice después de la semilla\n",
    "    caracteres_desde_semilla = respuesta_completa[primer_indice_despues_de_semilla:]\n",
    "\n",
    "    return respuesta_completa, caracteres_desde_semilla\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yFKCEGOYIYnd",
    "outputId": "c7cf9d8a-dd6c-4262-9839-e5b2fd4708d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Respuesta completa: Señor Sánchez, sus recetas económicas son tan creíbles como sus promesas electorales, y encima proponemos pagar de inmediato los ERTE pendientes y ampliaron tearquier y a los radicales que incendian Barcelona una vez más. Menudo reencuentro. ¿Cuándo piensa reconocer algún error y pedir disculpas por qué \n",
      "\n",
      "\n",
      "Caracteres desde el primer carácter después de la semilla:  Sánchez, sus recetas económicas son tan creíbles como sus promesas electorales, y encima proponemos pagar de inmediato los ERTE pendientes y ampliaron tearquier y a los radicales que incendian Barcelona una vez más. Menudo reencuentro. ¿Cuándo piensa reconocer algún error y pedir disculpas por qué \n"
     ]
    }
   ],
   "source": [
    "# Ejemplo de uso\n",
    "respuesta, caracteres_mostrados = generar_respuesta(one_step_casado, 'Señor')\n",
    "print(f\"Respuesta completa: {respuesta}\")\n",
    "print(f\"\\n\\nCaracteres desde el primer carácter después de la semilla: {caracteres_mostrados}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kvnNASr4agtN"
   },
   "source": [
    "\n",
    "Aqui se inicia un diálogo utilizando una semilla y los 3 modelos anteriores(one_step_abascal, one_step_casado, one_step_sanchez). Luego, realiza un bucle para continuar el diálogo durante un número especificado de iteraciones.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F2D1vxxNAq1N",
    "outputId": "6dd8ca7e-5064-475e-cbc1-87bdf5bdbb12"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semilla: buenos días\n",
      "------\n",
      " Respuesta de Abascal: buenos días. Señor Sánchez, ¿se reunió el ministro de Transporte de contral ros por bor adomas, con la lasie se enfravie tembién da la Corandi cod el nonos de der cobraro que no es lo que hoy a social hon perdido el contacto con la calle, que no están en los mercados, en los bares. Porque si estuvieran, sabría\n",
      "\n",
      "-----------Iteración:2\n",
      "\n",
      "------Respuesta de Casado:  Tonal. Dice que va a poner peajes en las carreteras. Por cierto, ¿puede confirmar si eso es la población de anteayer, podemos hacerlo.\n",
      "Acabo, señora presidenta. … la Usted ha conseguido una investidura, podrá conseguir los presupuestos, podrá conseguir varias leyes ―sobre más escándalos —‌y acabo y\n",
      "\n",
      "------Respuesta de Sanchez:  tendido. Y tirme de la mayoría parlamentaria en Madrid, la comunidad le tiendo la manoría de su grupo parlamentario. De hecho, creo que todos podemos convenir en que hemos logrado en contra de los jubito en el Primer lugar, demper delante médita que el Gobierno de España está por la unidad, y esl e\n",
      "\n",
      "------Respuesta de Abascal: n infortado de ello. ¿Sabe usted que el terrorista, que el asesino ha sido un exMENA? ¿Sabe usted, señor Sánchez.\n",
      "Muy buenos días. Señor Sánchez, ¿se reunió el ministro de Transporte de la electricidad y del gas. Usted apuesta por un modelo de decrecimiento y frivolidades mientras los españoles sufr\n",
      "\n",
      "Run time: 2.990095615386963\n",
      "\n",
      "-----------Iteración:2\n",
      "\n",
      "------Respuesta de Casado: es de todo menos el ridículo; su penoso monólogo de veintinueve segunda ol asesinoado de la destrucación de la que le convivencia y el respeto a las leyes, pero que aplaudirle políticos, con la facilita la investidura de julio, recuerde cómo ha tratado a Felipe González, al señor Rubalcaba, a Sánche\n",
      "\n",
      "------Respuesta de Sanchez: , que han negocido a su partido, aconómico y la sociedad española, pero también de la ciencia. En definitiva, yo creo que es más o menos que tenemos por delante. Lo hicimos así en el ámbito del diálogo social, pero no me hace una comisión intervencionando también el sector turístico, señoría, ¡qué q\n",
      "\n",
      "------Respuesta de Abascal: ue las delincuentes ya están de le juecca a las caladaros, sufren las clases populares y sufren las clases medias.\n",
      "Señor Sánchez, la fato cuend a las infestados sin lie gasto al señor Casado en lugar de a mí y mintiendo sobre todas nuestro país se ociarda a partidos meneraciones, de vivir peor que s\n",
      "\n",
      "Run time: 4.111233949661255\n"
     ]
    }
   ],
   "source": [
    "# Iniciar diálogo\n",
    "semilla_inicial = 'buenos días'\n",
    "num_iteraciones = 2  # Puedes ajustar este valor según tus necesidades\n",
    "print(f\"Semilla: {semilla_inicial}\")\n",
    "# Respuestas iniciales\n",
    "respuesta_abascal, caracteres_mostrados = generar_respuesta(one_step_abascal, 'buenos días')\n",
    "print(f\"------\\n Respuesta de Abascal: {respuesta_abascal}\")\n",
    "\n",
    "# Bucle para continuar el diálogo\n",
    "for i in range(num_iteraciones):\n",
    "    start_time = time.time()\n",
    "\n",
    "    print(f\"\\n-----------Iteración:{num_iteraciones}\")\n",
    "\n",
    "    respuesta_casado, caracteres_mostrados = generar_respuesta(one_step_casado, respuesta_abascal)\n",
    "    print(f\"\\n------Respuesta de Casado: {caracteres_mostrados}\")\n",
    "\n",
    "    respuesta_sanchez, caracteres_mostrados = generar_respuesta(one_step_sanchez, respuesta_casado)\n",
    "    print(f\"\\n------Respuesta de Sanchez: {caracteres_mostrados}\")\n",
    "\n",
    "    respuesta_abascal, caracteres_mostrados = generar_respuesta(one_step_abascal, respuesta_sanchez)\n",
    "    print(f\"\\n------Respuesta de Abascal: {caracteres_mostrados}\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    \n",
    "    print('\\nRun time:', end_time - start_time)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KvdlYgQB3nn0"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
